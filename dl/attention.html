
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub 📖" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>13. Attention Layers &#8212; deep learning for molecules &amp; materials</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dmol.pub/dl/attention.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14. Deep Learning on Sequences" href="NLP.html" />
    <link rel="prev" title="12. Explaining Predictions" href="xai.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">deep learning for molecules & materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   3. Regression &amp; Model Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   4. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/kernel.html">
   5. Kernel Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   6. Deep Learning Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   7. Standard Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gnn.html">
   8. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   9. Input Data &amp; Equivariances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Equivariant.html">
   10. Equivariant Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="molnets.html">
   11. Modern Molecular NNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xai.html">
   12. Explaining Predictions
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   13. Attention Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   14. Deep Learning on Sequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VAE.html">
   15. Variational Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="flows.html">
   16. Normalizing Flows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  D. Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/QM9.html">
   17. Predicting DFT Energies with GNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/MolGenerator.html">
   18. Generative RNN in Browser
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  E. Contributed Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Hyperparameter_tuning.html">
   19. Hyperparameter Tuning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  F. Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../style.html">
   20. Style Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   21. Changelog
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <script async defer src="https://api.dmol.pub/latest.js"></script><noscript><img src="https://api.dmol.pub/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript> By <a href="https://twitter.com/andrewwhite01">Andrew White</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/dl/attention.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/whitead/dmol-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fdl/attention.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/attention.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   13.1. Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-mechanism-equation">
   13.2. Attention Mechanism Equation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-reduction">
   13.3. Attention Reduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-dot">
   13.4. Tensor-Dot
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-hard-and-temperature-attention">
   13.5. Soft, Hard, and Temperature Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-attention">
   13.6. Self-Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trainable-attention">
   13.7. Trainable Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-head-attention-block">
   13.8. Multi-head Attention Block
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   13.9. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-examples">
   13.10. Code Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensor-dot-mechanism">
     13.10.1. Tensor-Dot Mechanism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-attention">
     13.10.2. General Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     13.10.3. Self-attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-trainable-parameters">
     13.10.4. Adding Trainable Parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-head">
     13.10.5. Multi-head
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-in-graph-neural-networks">
   13.11. Attention in Graph Neural Networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   13.12. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   13.13. Cited References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Attention Layers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   13.1. Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-mechanism-equation">
   13.2. Attention Mechanism Equation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-reduction">
   13.3. Attention Reduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-dot">
   13.4. Tensor-Dot
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#soft-hard-and-temperature-attention">
   13.5. Soft, Hard, and Temperature Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-attention">
   13.6. Self-Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trainable-attention">
   13.7. Trainable Attention
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-head-attention-block">
   13.8. Multi-head Attention Block
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   13.9. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-examples">
   13.10. Code Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensor-dot-mechanism">
     13.10.1. Tensor-Dot Mechanism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-attention">
     13.10.2. General Attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     13.10.3. Self-attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-trainable-parameters">
     13.10.4. Adding Trainable Parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-head">
     13.10.5. Multi-head
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-in-graph-neural-networks">
   13.11. Attention in Graph Neural Networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   13.12. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   13.13. Cited References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="attention-layers">
<h1><span class="section-number">13. </span>Attention Layers<a class="headerlink" href="#attention-layers" title="Permalink to this headline">¶</a></h1>
<p>Attention is a concept in machine learning and AI that goes back many years, especially in computer vision<span id="id1">[<a class="reference internal" href="#id59" title="Shumeet Baluja and Dean A. Pomerleau. Expectation-based selective attention for visual monitoring and control of a robot vehicle. Robotics and Autonomous Systems, 22(3):329–344, 1997. Robot Learning: The New Wave. URL: http://www.sciencedirect.com/science/article/pii/S0921889097000468, doi:https://doi.org/10.1016/S0921-8890(97)00046-8.">BP97</a>]</span>. Like the word “neural network”, attention was inspired by the idea of attention in how human brains deal with the massive amount of visual and audio input<span id="id2">[<a class="reference internal" href="#id58" title="Anne M Treisman and Garry Gelade. A feature-integration theory of attention. Cognitive psychology, 12(1):97–136, 1980.">TG80</a>]</span>. <strong>Attention layers</strong> are deep learning layers that evoke the idea of attention. You can read more about attention in deep learning in Luong et al. <span id="id3">[<a class="reference internal" href="#id56" title="Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.">LPM15</a>]</span> and get a practical <a class="reference external" href="http://d2l.ai/chapter_attention-mechanisms/index.html">overview here</a>. Attention layers have been empirically shown to be so effective in modeling sequences, like language, that they have become indispensible<span id="id4">[<a class="reference internal" href="#id55" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 5998–6008. 2017.">VSP+17</a>]</span>. The most common place you’ll see attention layers is in <a class="reference external" href="http://d2l.ai/chapter_attention-mechanisms/transformer.html"><strong>transformer</strong></a> neural networks that model sequences. We’ll also sometimes see attention in graph neural networks.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Attention can be confusing because of the three inputs. We’ll see later though that these inputs are actually often identical. The query is one key and the keys and values are equal. Then if you batch the queries together (one for each key), then you’ll see the query, keys, and values are equal. This is self-attention.</p>
</aside>
<div class="admonition-audience-objectives admonition">
<p class="admonition-title">Audience &amp; Objectives</p>
<p>This chapter builds on <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> and <a class="reference internal" href="../math/tensors-and-shapes.html"><span class="doc">Tensors and Shapes</span></a>. You should be comfortable with broadcasting, matrices, and tensor shapes. After completing this chapter, you should be able to</p>
<ul class="simple">
<li><p>Correctly specify shapes and input/output of attention layers</p></li>
<li><p>Implement attention layers</p></li>
<li><p>Understand how attention can be put into other layer types</p></li>
</ul>
</div>
<p>Attention layers are fundamentally a weighted mean reduction. It is just computing a mean, where you somehow weight each element contributing to the mean. Since it is a mean, attention decreases the rank of an input tensor. Attention is unusual among layers because it takes three inputs, whereas most layers in deep learning take just one or perhaps two. These inputs are called the <strong>query</strong>, the <strong>values</strong>, and the <strong>keys</strong>. The reduction occurs over the values; so if the values are rank 3, the output will be rank 2. The query should be one less rank than
the keys. The keys should be the same rank as the values. The keys and query determine how to weight the values according the <strong>attention mechanism</strong> – a fancy word for equation.</p>
<p>The table below summarizes these three input arguments. Note that often the query is batched, so that its rank will be 2 if batched. If the input query is batched, then the output’s rank will be batched as well and be 2 instead of 1.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>Rank</p></th>
<th class="head"><p>Axes</p></th>
<th class="head"><p>Purpose</p></th>
<th class="text-align:right head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Query</p></td>
<td><p>1</p></td>
<td><p>(# of attn features)</p></td>
<td><p>input for checking against keys</p></td>
<td class="text-align:right"><p>One word represented as feature vector</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Keys</p></td>
<td><p>2</p></td>
<td><p>(sequence length, # of attn features)</p></td>
<td><p>used to compute attention against query</p></td>
<td class="text-align:right"><p>All words in a sentence represented as matrix of feature vectors</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Values</p></td>
<td><p>2</p></td>
<td><p>(sequence length, # of value features)</p></td>
<td><p>used to compute value of output</p></td>
<td class="text-align:right"><p>A vector of numbers for each word in a sentence</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Output</p></td>
<td><p>1</p></td>
<td><p>(# of value features)</p></td>
<td><p>attention-weighted mean over values</p></td>
<td class="text-align:right"><p>single vector</p></td>
</tr>
</tbody>
</table>
<section id="example">
<h2><span class="section-number">13.1. </span>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>Attention is best conceptualized as operating on a sequence. Let’s use a sentence like “The sleepy child reads a book”. The words in the sentence correspond to the keys. If we represent our words as embeddings, our keys will be rank 2. For example, the word “sleepy” might be represented by an embedding vector of length 2: <span class="math notranslate nohighlight">\([2,0,1]\)</span>, where these embedding values are trained or taken from a standard language embedding. By convention, the zeroth axis of keys will be the position in the sequence and the first axis contains these vectors. The query is often an element from the keys, like the word “book.” The point of attention is to see what parts of the sentence the query should be influenced by. “Book” should have strong attention on “child” and “reads,” but probably not to “sleepy.” You’ll see soon that we will actually compute this as a vector, called the attention vector <span class="math notranslate nohighlight">\(\vec{b}\)</span>. The output from the attention layer will be a reduction over the values where each element of values is weighted by the attention between the query and the key. Thus there should be one key and one value for each element in our sentence. The values could be identical to the keys, which is common.</p>
<p>Let’s see how this looks mathematically. The attention layer consists of two steps: (1) computing the attention vector <span class="math notranslate nohighlight">\(\vec{b}\)</span> using the <strong>attention mechanism</strong> and (2) the reduction over the values using the attention vector <span class="math notranslate nohighlight">\(\vec{b}\)</span>. Attention mechanism is a fancy word for the attention equation. Consider our example above. We’ll use a 3-dimensional embedding for our words</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Keys and queries as one-hot encodings will not work as inputs to attention layers because the dot product will give zeros, unless the key and query are equal.</p>
</aside>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Index</p></th>
<th class="text-align:center head"><p>Embedding</p></th>
<th class="text-align:right head"><p>Word</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>0</p></td>
<td class="text-align:center"><p>0,0,0</p></td>
<td class="text-align:right"><p>The</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>1</p></td>
<td class="text-align:center"><p>2,0,1</p></td>
<td class="text-align:right"><p>Sleepy</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>2</p></td>
<td class="text-align:center"><p>1,-1,-2</p></td>
<td class="text-align:right"><p>Child</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>3</p></td>
<td class="text-align:center"><p>2,3,1</p></td>
<td class="text-align:right"><p>Reads</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>4</p></td>
<td class="text-align:center"><p>-2,0,0</p></td>
<td class="text-align:right"><p>A</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>5</p></td>
<td class="text-align:center"><p>0,2,1</p></td>
<td class="text-align:right"><p>Book</p></td>
</tr>
</tbody>
</table>
<p>The keys will be a rank 2 tensor (matrix) putting all these together. Note that these are only integers to make this example clearer, typically words are represented with floating point numbers when embedded.</p>
<div class="amsmath math notranslate nohighlight" id="equation-44e962cc-476c-49f6-934e-078e2e102cd5">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-44e962cc-476c-49f6-934e-078e2e102cd5" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{K} = \left[
\begin{array}{lccccr}
0 &amp; 2 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\
0 &amp; 0 &amp; -1 &amp; 3 &amp; 0 &amp; 2\\
0 &amp; 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1\\
\end{array}\right]
\end{equation}\]</div>
<p>They keys are shape <span class="math notranslate nohighlight">\((6, 3)\)</span> because our sentence has 6 words and each word is represented with a 3 dimensional embedding vector. Let’s make our values simple, we’ll have one for each word. These values are what determine our output. Perhaps they could be the sentiment of the word: is it a positive word (“happy”) or a negative word (“angry”).</p>
<div class="amsmath math notranslate nohighlight" id="equation-328eba98-b195-4dea-aee3-ee7c8dc9972c">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-328eba98-b195-4dea-aee3-ee7c8dc9972c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{V} = \left[ 0, -0.2, 0.3, 0.4, 0, 0.1\right]
\end{equation}\]</div>
<p>Note that the values <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> should be the same rank as the keys, so its shape is interpreted as <span class="math notranslate nohighlight">\((6, 1)\)</span>. Finally, the query which should be one rank less than the keys. Our query is the word “book:”</p>
<div class="amsmath math notranslate nohighlight" id="equation-1026363c-801a-4c10-88fb-0711d43d6e13">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-1026363c-801a-4c10-88fb-0711d43d6e13" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{q} = \left[0, 2, 1\right]
\end{equation}\]</div>
</section>
<section id="attention-mechanism-equation">
<h2><span class="section-number">13.2. </span>Attention Mechanism Equation<a class="headerlink" href="#attention-mechanism-equation" title="Permalink to this headline">¶</a></h2>
<p>The attention mechanism equation uses query and keys arguments only. It outputs a tensor one rank less than the keys, giving a scalar for each key corresponding to the attention the query should have for the key. This attention vector should be normalized. The most common attention mechanism a dot product and softmax:</p>
<div class="amsmath math notranslate nohighlight" id="equation-afec14e1-5de5-489b-a864-03b279100625">
<span class="eqno">(13.4)<a class="headerlink" href="#equation-afec14e1-5de5-489b-a864-03b279100625" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{b} = \mathrm{softmax}\left(\vec{q}\cdot \mathbf{K}\right) = \mathrm{softmax}\left(\sum_j q_j k_{ij}\right)
\end{equation}\]</div>
<p>where index <span class="math notranslate nohighlight">\(i\)</span> is the position in the sequence and <span class="math notranslate nohighlight">\(j\)</span> is the index of the feature. Softmax is defined as</p>
<div class="math notranslate nohighlight" id="equation-softmax">
<span class="eqno">(13.5)<a class="headerlink" href="#equation-softmax" title="Permalink to this equation">¶</a></span>\[\mathrm{softmax}\left(\vec{x}\right) = \frac{e^\vec{x}}{\sum_i e^ x_i}\]</div>
<p>and ensures that <span class="math notranslate nohighlight">\(\vec{b}\)</span> is normalized. Substituting our values from the example above:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e1daedd-1805-4ffc-9363-efc66f08b704">
<span class="eqno">(13.6)<a class="headerlink" href="#equation-6e1daedd-1805-4ffc-9363-efc66f08b704" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{b} = \mathrm{softmax}\left(\left[0, 2, 1\right] \times
\left[
\begin{array}{lccccr}
0 &amp; 2 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\
0 &amp; 0 &amp; -1 &amp; 3 &amp; 0 &amp; 2\\
0 &amp; 1 &amp; -2 &amp; 1 &amp; 0 &amp; 1\\
\end{array}\right]\right) = \mathrm{softmax}\left( \left[0, 1, -4, 7, 0, 5\right]\right)
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-2537d5b2-f133-495d-a251-058368275539">
<span class="eqno">(13.7)<a class="headerlink" href="#equation-2537d5b2-f133-495d-a251-058368275539" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{b}  = \left[0, 0, 0, 0.88, 0, 0.12\right]
\end{equation}\]</div>
<p>I’ve rounded the numbers here, but essentially the attention vector only gives weight to the word itself (book) and the verb “read”. I made this up, remember, but it gives you an idea of how attention gives you a way to connect words. It may even remind you of our graph neural network’s idea of neighbors.</p>
</section>
<section id="attention-reduction">
<h2><span class="section-number">13.3. </span>Attention Reduction<a class="headerlink" href="#attention-reduction" title="Permalink to this headline">¶</a></h2>
<p>After computing the attention vector <span class="math notranslate nohighlight">\(\vec{b}\)</span>, this is used to compute a weighted mean over the values:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4ac07c09-4be2-4530-8eae-16934f168bbb">
<span class="eqno">(13.8)<a class="headerlink" href="#equation-4ac07c09-4be2-4530-8eae-16934f168bbb" title="Permalink to this equation">¶</a></span>\[\begin{equation}
 \mathbf{V}\vec{b} = \left[0, 0, 0, 0.88, 0, 0.12\right]^ T \left[ 0, -0.2, 0.3, 0.4, 0, 0.1\right] = 0.36
\end{equation}\]</div>
<p>Conceptually, our example computed the attention-weighted sentiment of the query word “book” in our sentence. You can see that attention layers do two things: compute an attention vector with the attention mechanism and then use it to take the attention-weighted average over the values.</p>
</section>
<section id="tensor-dot">
<h2><span class="section-number">13.4. </span>Tensor-Dot<a class="headerlink" href="#tensor-dot" title="Permalink to this headline">¶</a></h2>
<p>This dot product, softmax, and reduction is called a tensor-dot and is the most common attention layer<span id="id5">[<a class="reference internal" href="#id56" title="Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.">LPM15</a>]</span>. One common modification is to divide by the dimension of the keys (last axis dimension). Remember the keys are not normalized. If they are random numbers, the magnitude of the output from the dot product scales with the square root of the dimension of the keys due to the central limit theorem. This can make the soft-max behave poorly, since you’re taking <span class="math notranslate nohighlight">\(e^{\vec{q} \cdot \mathbf{K}}\)</span>. Putting this all together, the equation is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c847598c-e174-4459-9dca-b7ce68eb28d0">
<span class="eqno">(13.9)<a class="headerlink" href="#equation-c847598c-e174-4459-9dca-b7ce68eb28d0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \vec{b} = \mathrm{softmax}\left(\frac{1}{\sqrt{d}}\vec{q}\cdot \mathbf{K}\right)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the dimension of the query vector.</p>
</section>
<section id="soft-hard-and-temperature-attention">
<h2><span class="section-number">13.5. </span>Soft, Hard, and Temperature Attention<a class="headerlink" href="#soft-hard-and-temperature-attention" title="Permalink to this headline">¶</a></h2>
<p>One possible change to attention is to replace the <span class="math notranslate nohighlight">\(\mathrm{softmax}\)</span> with a one at the position of highest attention and zero at all others. This is called <strong>hard attention</strong>. The equation for hard attention is to replace softmax with a “hardmax”, defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-79f27d74-0c27-4e8c-970f-9f86e8218771">
<span class="eqno">(13.10)<a class="headerlink" href="#equation-79f27d74-0c27-4e8c-970f-9f86e8218771" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathrm{hardmax}\left(\vec{x}\right) = \lim_{T\rightarrow0}\frac{e^\vec{x} / T}{\sum_i e^ {x_i / T}}
\end{equation}\]</div>
<p>which is a mathematical way to formulate putting <span class="math notranslate nohighlight">\(1\)</span> in the position of the largest element of <span class="math notranslate nohighlight">\(\vec{x}\)</span> and a <span class="math notranslate nohighlight">\(0\)</span> at all others. The choice of <span class="math notranslate nohighlight">\(T\)</span> is for the word temperature, because this equation is similar to Boltzmann’s distribution from statistical mechanics. You can see that limit <span class="math notranslate nohighlight">\(T = 0\)</span> is the hard attention, <span class="math notranslate nohighlight">\(T = 1\)</span> is the soft attention, and <span class="math notranslate nohighlight">\(T = \infty\)</span> means uniform attention. you could tune <span class="math notranslate nohighlight">\(T\)</span> to some intermediate values as well.</p>
</section>
<section id="self-attention">
<h2><span class="section-number">13.6. </span>Self-Attention<a class="headerlink" href="#self-attention" title="Permalink to this headline">¶</a></h2>
<p>Remember how everything is batched in deep learning? The batched input to an attention layer is usually the query. So although in the above discussion it was a tensor of one rank less than the keys (typically a query <em>vector</em>), once it has been batched it will be the same rank as the keys. Almost always, the query is in fact equal to the keys. Like in our example, our query was the embedding vector for the word “book”, which is one of the keys. If you consider the query to be batched so that you consider every word in the sentence, the query becomes equal to the keys. A further special case is when the query, values and keys are equal. This is called <strong>self-attention</strong>. This just means our attention mechanism uses the values directly and there is no extra set of “keys” input to the layer.</p>
</section>
<section id="trainable-attention">
<h2><span class="section-number">13.7. </span>Trainable Attention<a class="headerlink" href="#trainable-attention" title="Permalink to this headline">¶</a></h2>
<p>There are no trainable parameters in our definitions above. How can you do learning with attention? Typically, you don’t have trainable parameters in equations directly. Instead, you put the keys, values, and query through a dense layer (see <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a>) before the attention. So when viewed as a layer, attention has no trainable parameters. When viewed as a block with a dense layer and attention layer, it is trainable. We’ll see this now explicitly below.</p>
</section>
<section id="multi-head-attention-block">
<h2><span class="section-number">13.8. </span>Multi-head Attention Block<a class="headerlink" href="#multi-head-attention-block" title="Permalink to this headline">¶</a></h2>
<p>Inspired by the idea of convolutions with multiple filters, there is a block (group of layers) that splits to multiple parallel attentions.  These are called “multi-head attention”. If your values are shape <span class="math notranslate nohighlight">\((L, V)\)</span>, you will get back a <span class="math notranslate nohighlight">\((H, V)\)</span> tensor, where <span class="math notranslate nohighlight">\(H\)</span> is the number of parallel attention layers (heads). If there are no trainable parameters in attention layers, what’s the point of this though? Well, you must introduce weights. These are <em>square</em> weight matrices because we need all shapes to remain constant among all the attention heads.</p>
<p>Consider an attention layer to be defined by <span class="math notranslate nohighlight">\(A(\vec{q}, \mathbf{K}, \mathbf{V})\)</span>. The multi-head attention is</p>
<div class="amsmath math notranslate nohighlight" id="equation-d4858fb4-726d-4cd9-8e67-2ee511635d95">
<span class="eqno">(13.11)<a class="headerlink" href="#equation-d4858fb4-726d-4cd9-8e67-2ee511635d95" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\left[A(\mathbf{W}_q^0\vec{q}, \mathbf{W}_k^0\mathbf{K}, \mathbf{W}_v^0\mathbf{V}), A(\mathbf{W}_q^1\vec{q}, \mathbf{W}_k^1\mathbf{K}, \mathbf{W}_v^1\mathbf{V}), \ldots, A(\mathbf{W}_q^H\vec{q}, \mathbf{W}_k^H\mathbf{K}, \mathbf{W}_v^H\mathbf{V})\right]
\end{equation}\]</div>
<p>where each element of the output vector <span class="math notranslate nohighlight">\([\ldots]\)</span> is itself an output vector from an attention layer, making <span class="math notranslate nohighlight">\(H\)</span>  <span class="math notranslate nohighlight">\((L, V)\)</span> shaped tensors. So the whole output is an <span class="math notranslate nohighlight">\((H, L, V)\)</span> tensor. The most famous example of the multi-head attention block is in transformers<span id="id6">[<a class="reference internal" href="#id55" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 5998–6008. 2017.">VSP+17</a>]</span> where they use self-attention multi-head attention blocks.</p>
<p>Typically we apply multiple sequential blocks of attention, so need the values input to the next block to be of rank 2 again (instead of the rank 3 <span class="math notranslate nohighlight">\((H, L, V)\)</span> tensor). Thus the output from the multi-head attention is often reduced by matrix multiplication with an <span class="math notranslate nohighlight">\((H, V, V)\)</span> weight tensor or a <span class="math notranslate nohighlight">\((H)\)</span> tensor of weights so that you get back to rank 2. If this seems confusing, see the example below.</p>
</section>
<section id="running-this-notebook">
<h2><span class="section-number">13.9. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Permalink to this headline">¶</a></h2>
<p>Click the  <i aria-label="Launch interactive content" class="fas fa-rocket"></i>  above to launch this page as an interactive Google Colab.</p>
</section>
<section id="code-examples">
<h2><span class="section-number">13.10. </span>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h2>
<p>Let’s see how attention can be implemented in code. I will use random variables here for the different quantities but I will indicate which variables should be trained with <code class="docutils literal notranslate"><span class="pre">w_</span></code> and which should be inputs with <code class="docutils literal notranslate"><span class="pre">i_</span></code>.</p>
<section id="tensor-dot-mechanism">
<h3><span class="section-number">13.10.1. </span>Tensor-Dot Mechanism<a class="headerlink" href="#tensor-dot-mechanism" title="Permalink to this headline">¶</a></h3>
<p>We’ll begin with implementing the tensor-dot attention mechanism first. As an example, we’ll use a sequence length of 11 and a keys feature length of 4 and a values feature dimension of 2. Remember the keys and query must share feature dimension size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tensor_dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">((</span><span class="n">k</span> <span class="o">@</span> <span class="n">q</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">b</span>


<span class="n">i_query</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="n">i_keys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">tensor_dot</span><span class="p">(</span><span class="n">i_query</span><span class="p">,</span> <span class="n">i_keys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b = &quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b =  [0.01433916 0.02343355 0.0526661  0.05773857 0.01991906 0.4861591
 0.02880975 0.04713923 0.01556142 0.21919564 0.03503842]
</pre></div>
</div>
</div>
</div>
<p>As expected, we get out a vector <span class="math notranslate nohighlight">\(\vec{b}\)</span> whose sum is 1.</p>
</section>
<section id="general-attention">
<h3><span class="section-number">13.10.2. </span>General Attention<a class="headerlink" href="#general-attention" title="Permalink to this headline">¶</a></h3>
<p>Now let’s put this attention mechanism into an attention layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">attention_layer</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tensor_dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span> <span class="o">@</span> <span class="n">v</span>


<span class="n">i_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">attention_layer</span><span class="p">(</span><span class="n">i_query</span><span class="p">,</span> <span class="n">i_keys</span><span class="p">,</span> <span class="n">i_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.06082633, -0.57622567])
</pre></div>
</div>
</div>
</div>
<p>We get two values, one for each feature dimension.</p>
</section>
<section id="id7">
<h3><span class="section-number">13.10.3. </span>Self-attention<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>The change in self-attention is that we make queries, keys, and values equal. We need to make a small change in that the queries are batched in this setting, so we should get a rank 2 output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">batched_tensor_dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="c1"># a will be batch x seq x feature dim</span>
    <span class="c1"># which is N x N x 4</span>
    <span class="c1"># batched dot product in einstein notation</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ij,kj-&gt;ik&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># now we softmax over sequence</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>


<span class="k">def</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">batched_tensor_dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span> <span class="o">@</span> <span class="n">x</span>


<span class="n">i_batched_query</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">self_attention</span><span class="p">(</span><span class="n">i_batched_query</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.0962209 ,  0.625929  ,  0.48575186, -0.55562792],
       [ 0.28604869,  0.1297031 ,  0.49199019, -0.54365314],
       [-0.2441835 ,  0.8718151 , -0.17742252,  0.11112741],
       [ 0.00996486,  0.17003543,  0.03411565,  0.03201869],
       [-0.63903385,  0.88532876, -0.14997991,  0.12800388],
       [-0.26946665,  0.2562351 , -1.11294878,  1.06651085],
       [ 0.14797865,  1.69737329, -0.92178528,  1.67189519],
       [-0.15116729,  0.32392897, -0.16989323,  0.23688997],
       [-0.24933373,  1.29483436, -0.26031842,  0.30741714],
       [-0.10702615,  0.40590462, -0.83854546,  1.3221695 ],
       [-0.06817569,  0.33934936, -0.11949148,  0.05728925]])
</pre></div>
</div>
</div>
</div>
<p>We are given as output an <span class="math notranslate nohighlight">\(11\times4\)</span> matrix, which is correct.</p>
</section>
<section id="adding-trainable-parameters">
<h3><span class="section-number">13.10.4. </span>Adding Trainable Parameters<a class="headerlink" href="#adding-trainable-parameters" title="Permalink to this headline">¶</a></h3>
<p>You can add trainable parameters to these steps by adding a weight matrix. Let’s do this for the self-attention. Although keys, values, and query are equal in self-attention, I can multiply them by different weights. Just to demonstrate, I’ll have the values change to feature dimension 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># weights should be input feature_dim -&gt; desired output feature_dim</span>
<span class="n">w_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">trainable_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_q</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_k</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_v</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">batched_tensor_dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span> <span class="o">@</span> <span class="n">v</span>


<span class="n">trainable_self_attention</span><span class="p">(</span><span class="n">i_batched_query</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-9.05789522e-02,  1.69701960e-01],
       [ 9.25796763e-01, -1.42131499e+00],
       [-7.45441203e-01,  1.32038619e+00],
       [ 3.32482735e-01,  3.21884752e-01],
       [-3.81943703e+00,  7.71199904e+00],
       [ 5.74819349e-01,  7.41025733e-01],
       [-6.44089880e+01,  1.26365784e+02],
       [-7.99730076e-01,  2.69528298e+00],
       [-4.61035328e+00,  9.48299554e+00],
       [-1.98622553e+01,  6.40354007e+01],
       [ 1.67863333e-02, -5.62813900e-02]])
</pre></div>
</div>
</div>
</div>
<p>Since we had our values change to feature dimension 2 with the weights, we get out an <span class="math notranslate nohighlight">\(11\times 2\)</span> output.</p>
</section>
<section id="multi-head">
<h3><span class="section-number">13.10.5. </span>Multi-head<a class="headerlink" href="#multi-head" title="Permalink to this headline">¶</a></h3>
<p>The only change for multi-head attention is that we have one set of weights for each head and we agree on how to combine after running through the heads. I’ll just use a length <span class="math notranslate nohighlight">\(H\)</span> vector of trainable weights. Other strategies are to concatenate them or use a reduction (e.g., mean, max).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_q_h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_k_h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_v_h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">w_q_h2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_k_h2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">w_v_h2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">w_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">multihead_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q_h1</span><span class="p">,</span> <span class="n">w_k_h1</span><span class="p">,</span> <span class="n">w_v_h1</span><span class="p">,</span> <span class="n">w_q_h2</span><span class="p">,</span> <span class="n">w_k_h2</span><span class="p">,</span> <span class="n">w_v_h2</span><span class="p">):</span>
    <span class="n">h1_out</span> <span class="o">=</span> <span class="n">trainable_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q_h1</span><span class="p">,</span> <span class="n">w_k_h1</span><span class="p">,</span> <span class="n">w_v_h1</span><span class="p">)</span>
    <span class="n">h2_out</span> <span class="o">=</span> <span class="n">trainable_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_q_h2</span><span class="p">,</span> <span class="n">w_k_h2</span><span class="p">,</span> <span class="n">w_v_h2</span><span class="p">)</span>
    <span class="c1"># join along last axis so we can use dot.</span>
    <span class="n">all_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">h1_out</span><span class="p">,</span> <span class="n">h2_out</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_h</span> <span class="o">@</span> <span class="n">w_h</span>


<span class="n">multihead_attention</span><span class="p">(</span><span class="n">i_batched_query</span><span class="p">,</span> <span class="n">w_q_h1</span><span class="p">,</span> <span class="n">w_k_h1</span><span class="p">,</span> <span class="n">w_v_h1</span><span class="p">,</span> <span class="n">w_q_h2</span><span class="p">,</span> <span class="n">w_k_h2</span><span class="p">,</span> <span class="n">w_v_h2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-6.72381605e+02, -5.86830005e+02],
       [-8.20597468e+00, -7.08023050e-01],
       [-6.51471573e-01, -9.61627384e-03],
       [-1.20959809e-01, -3.47849853e-02],
       [-6.17380110e+03, -7.82281267e+03],
       [ 8.77933146e+00,  6.89962364e+00],
       [ 2.27580666e+02,  2.43570479e+02],
       [-2.45689103e-01, -1.00485438e-01],
       [-1.01543461e+00,  3.29492466e-01],
       [ 8.15300823e+00,  7.89955835e+00],
       [ 2.76144539e-01,  1.90124104e-01]])
</pre></div>
</div>
</div>
</div>
<p>As expected, we do get an <span class="math notranslate nohighlight">\(11\times2\)</span> rank 2 output.</p>
</section>
</section>
<section id="attention-in-graph-neural-networks">
<h2><span class="section-number">13.11. </span>Attention in Graph Neural Networks<a class="headerlink" href="#attention-in-graph-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Recall that the key attribute of a graph neural network is permutation equivariance. We used reductions like sum or mean over neighbors as the way to make the graph neural network layers be permutation equivariant. Attention layers are also permutation invariant (when not batched) and equivariant (when batched). This has made attention a popular choice for how to aggregate neighbor information. Attention layers are good at finding important neighbors and so are important with high-degree graphs (lots of neighbors). This is rare in molecules, but you can just define all atoms to be connected and then put distances as the edge attributes. Recall that graph convolution layers (GCN layer), and most GNN layers, only allow information to propagate one-bond per layer. Thus joining all atoms and using attention can give you long-range communication without so many layers. The disadvantage is that your network must now learn how to give attention to the correct bonds/atoms.</p>
<p>Let’s see how attention fits into the Battaglia equations<span id="id8">[<a class="reference internal" href="gnn.html#id74" title="Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.">BHB+18</a>]</span>. Recall that the Battaglia equations are general standard equations for defining a GNN. Attention can appear in multiple places, but as discussed above it appears when considering neighbors. Specifically, the query will be the <span class="math notranslate nohighlight">\(i\)</span>th node, and the keys/values will be some combination of neighboring node and edge features. There is no step in the Battaglia equations where this fits neatly, but we can split up the attention layer as follows. Most of the attention layer will fit into the edge update equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-77435ed8-2d2b-4c45-9c0d-fa932aaacebc">
<span class="eqno">(13.12)<a class="headerlink" href="#equation-77435ed8-2d2b-4c45-9c0d-fa932aaacebc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right)
\end{equation}\]</div>
<p>Recall that this is a general equation and our choice of <span class="math notranslate nohighlight">\(\phi^e()\)</span> defines the GNN. <span class="math notranslate nohighlight">\(\vec{e}_k\)</span> is the feature vector of edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> is the receiving node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> is the sending node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{u}\)</span> is the global graph feature vector. We will use this step for attention mechanism where the query is the receiving node <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> and the keys/values are composed of the senders and edges vectors. To be specific, we’ll use the approach from Zhang et al. <span id="id9">[<a class="reference internal" href="gnn.html#id80" title="Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.">ZSX+18</a>]</span> with a tensor-dot mechanism. They only considered node features and set the keys and values to be identical as the node features. However, they put trainable parameters at each layer that translated the node features in to the keys/query.</p>
<div class="amsmath math notranslate nohighlight" id="equation-b3728902-272a-46af-b3a1-7831a77eeb17">
<span class="eqno">(13.13)<a class="headerlink" href="#equation-b3728902-272a-46af-b3a1-7831a77eeb17" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{q} = \mathbf{W}_q\vec{v}_{rk}
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-01d100aa-884c-469a-8b4e-b352f51e1bd7">
<span class="eqno">(13.14)<a class="headerlink" href="#equation-01d100aa-884c-469a-8b4e-b352f51e1bd7" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{K} = \mathbf{W}_k\vec{v}_{sk}
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-029f5802-e236-4df1-8eea-e7f4efa5dc25">
<span class="eqno">(13.15)<a class="headerlink" href="#equation-029f5802-e236-4df1-8eea-e7f4efa5dc25" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{V} = \mathbf{W}_v\vec{v}_{sk}
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-717fc1ef-8d0e-4c6d-9616-11555ba25418">
<span class="eqno">(13.16)<a class="headerlink" href="#equation-717fc1ef-8d0e-4c6d-9616-11555ba25418" title="Permalink to this equation">¶</a></span>\[\begin{equation}
    \vec{b}_k = \mathrm{softmax}\left(\frac{1}{\sqrt{d}} \vec{q}\cdot \mathbf{K}\right)
\end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-57392a9f-d1aa-4509-8eb5-770aecdb7ae8">
<span class="eqno">(13.17)<a class="headerlink" href="#equation-57392a9f-d1aa-4509-8eb5-770aecdb7ae8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}^{'}_k = \vec{b} V
\end{equation}\]</div>
<p>Putting it compactly into one equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-298def04-0297-42b7-ae05-4b43bf8ddc51">
<span class="eqno">(13.18)<a class="headerlink" href="#equation-298def04-0297-42b7-ae05-4b43bf8ddc51" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}^{'}_k = \mathrm{softmax}\left(\frac{1}{\sqrt{d}} \mathbf{W}_q\vec{v}_{rk}\cdot \mathbf{W}_k\vec{v}_{sk}\right)\mathbf{W}_v\vec{v}_{sk}
\end{equation}\]</div>
<p>Now we have weighted edge feature vectors from the attention. Finally, we sum over these edge features in the edge aggregation step.</p>
<div class="amsmath math notranslate nohighlight" id="equation-22771fae-b7d2-4615-ba39-bb36d55f1619">
<span class="eqno">(13.19)<a class="headerlink" href="#equation-22771fae-b7d2-4615-ba39-bb36d55f1619" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right) = \sum E_i^{'}
\end{equation}\]</div>
<p>In Zhang et al. <span id="id10">[<a class="reference internal" href="gnn.html#id80" title="Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.">ZSX+18</a>]</span>, they used multi-headed attention as well. How would multi-headed attention work? Your edge feature matrix <span class="math notranslate nohighlight">\( E_i^{'}\)</span> now becomes an edge feature tensor, where axis 0 is edge (<span class="math notranslate nohighlight">\(k\)</span>), axis 1 is feature, and axis 2 is the head. Recall that the “head” just means which set of <span class="math notranslate nohighlight">\(\mathbf{W}^h_q, \mathbf{W}^h_k, \mathbf{W}^h_v\)</span> we used. To reduce the tensor back to the expected matrix, we simply use another weight matrix that maps from the last two axes (feature, head) down to features only. I will write-out the indices explicitly to be more clear:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9dfee471-f394-4a3d-9bf0-b70f5b3f8dc9">
<span class="eqno">(13.20)<a class="headerlink" href="#equation-9dfee471-f394-4a3d-9bf0-b70f5b3f8dc9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{e}^{'}_{il} = \rho^{e\rightarrow v}\left( E_i^{'}\right) = \sum_k e_{ikjh}^{'}w_{jhl}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(j\)</span> is edge feature input index, <span class="math notranslate nohighlight">\(l\)</span> is our output edge feature matrix, and <span class="math notranslate nohighlight">\(k,h,i\)</span> are defined as before. <strong>Transformer</strong> is another name for a network built on multi-headed attention, so you’ll also see transformer graph neural networks <span id="id11">[<a class="reference internal" href="#id87" title="Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.">MDM+20</a>]</span> building.</p>
</section>
<section id="chapter-summary">
<h2><span class="section-number">13.12. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Attention layers are inspired by human ideas of attention, but is fundamentally a weighted mean reduction.</p></li>
<li><p>The attention layer takes in three inputs: the query, the values, and the keys. These inputs are often identical, where the query is one key and the keys and the values are equal.</p></li>
<li><p>They are good at modeling sequences, such as language.</p></li>
<li><p>The attention vector should be normalized, which can be achieved using a softmax activation function, but the attention mechanism equation is a hyperparameter.</p></li>
<li><p>Attention layers compute an attention vector with the attention mechanism, and then reduce it by computing the attention-weighted average.</p></li>
<li><p>Using hard attention (hardmax function) returns the maximum output from the attention mechanism.</p></li>
<li><p>The tensor-dot followed by a softmax is the most common attention mechanism.</p></li>
<li><p>Self-attention is achieved when the query, values, and the keys are equal.</p></li>
<li><p>Attention layers by themselves are not trainable.</p></li>
<li><p>Multi-head attention block is a group of layers that splits to multiple parallel attentions.</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">13.13. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">¶</a></h2>
<div class="docutils container" id="id12">
<dl class="citation">
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id8">BHB+18</a></span></dt>
<dd><p>Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. <em>arXiv preprint arXiv:1806.01261</em>, 2018.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">ZSX+18</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. <em>arXiv preprint arXiv:1803.07294</em>, 2018.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id1">BP97</a></span></dt>
<dd><p>Shumeet Baluja and Dean A. Pomerleau. Expectation-based selective attention for visual monitoring and control of a robot vehicle. <em>Robotics and Autonomous Systems</em>, 22(3):329–344, 1997. Robot Learning: The New Wave. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0921889097000468">http://www.sciencedirect.com/science/article/pii/S0921889097000468</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/S0921-8890(97)00046-8">doi:https://doi.org/10.1016/S0921-8890(97)00046-8</a>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id2">TG80</a></span></dt>
<dd><p>Anne M Treisman and Garry Gelade. A feature-integration theory of attention. <em>Cognitive psychology</em>, 12(1):97–136, 1980.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">LPM15</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. <em>arXiv preprint arXiv:1508.04025</em>, 2015.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">VSP+17</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Advances in neural information processing systems</em>, 5998–6008. 2017.</p>
</dd>
<dt class="label" id="id87"><span class="brackets"><a class="fn-backref" href="#id11">MDM+20</a></span></dt>
<dd><p>Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. Molecule attention transformer. <em>arXiv preprint arXiv:2002.08264</em>, 2020.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="xai.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">12. </span>Explaining Predictions</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="NLP.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Deep Learning on Sequences</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Andrew D. White<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">✕</button> <img id="wh-modal-img"> </div>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>